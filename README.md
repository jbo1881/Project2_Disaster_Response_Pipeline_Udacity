# Project2_Disaster_Response_Pipeline_Udacity

## Table of Contents

- [Installation](#installation)
- [Project Motivation](#project-motivation)
- [Project Description](#project-description)
- [Files Description](#files-description)

## Installation

The libraries used are: 

  pandas: This library is used for data manipulation and analysis. It provides data structures like DataFrames and Series, making it easier to work with tabular data.
  
  numpy: NumPy is a fundamental package for scientific computing with Python. It provides support for arrays, matrices, and mathematical functions.
  
  sqlite3: This library is Python's built-in module for working with SQLite databases. It allows you to create, connect to, and interact with SQLite databases without the need for external installations.
  
  sqlalchemy: SQLAlchemy is a popular SQL toolkit and Object-Relational Mapping (ORM) library for Python. It's used for working with SQL databases more abstractly.
  
  sys: This module provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter. It is used to access command-line arguments.
  
  re: The re module is used for regular expressions, which are powerful tools for pattern matching and text processing.
  
  nltk: The Natural Language Toolkit is a library for working with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet. It's used for text processing and natural language processing (NLP) tasks.
  
  sklearn: Scikit-learn is a machine learning library for Python. It provides simple and efficient tools for data mining and data analysis. In the code, it is being used to create a machine learning model pipeline, perform data splitting, and evaluate the model's performance.
  
  pickle: Python's pickle module is used for serializing and deserializing Python objects. In the code, it's used to save and load machine learning models.



## Project Motivation

The project's objective is the categorization of disaster messages. To achieve this, I examined disaster data sourced from Appen with the aim of constructing an API model for message classification. Users can utilize a web application to input new messages and receive classifications across various categories. Additionally, the web app provides data visualizations for enhanced insights.

## Project Components

The project consists of the following components:

### 1. ETL Pipeline

In the `process_data.py` Python script, an ETL (Extract, Transform, Load) pipeline is implemented, which performs the following tasks:

- Loads the messages and categories datasets.
- Merges the two datasets.
- Cleans the data by transforming it into a structured format.
- Stores the cleaned data in a SQLite database.

### 2. ML Pipeline

The `train_classifier.py` Python script contains a machine learning pipeline that carries out the following steps:

- Loads data from the SQLite database generated by the ETL pipeline.
- Splits the dataset into training and test sets.
- Constructs a text processing and machine learning pipeline.
- Trains and fine-tunes a machine learning model using GridSearchCV.
- Evaluates and reports the model's performance on the test set.
- Exports the final model as a pickle file for future use.

### 3. Flask Web App

A web application is provided, allowing users to interact with the trained model. Key features of the web app include:

- Inputting a disaster message.
- Viewing the categories associated with the message.

 
  ## Files Description

